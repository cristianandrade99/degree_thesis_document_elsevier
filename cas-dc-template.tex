%% 
%% Copyright 2019-2020 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-dc documentclass for 
%% double column output.

%\documentclass[a4paper,fleqn,longmktitle]{cas-dc}
\documentclass[a4paper,fleqn]{cas-dc}

%\usepackage[authoryear,longnamesfirst]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[numbers]{natbib}

%%%Author definitions
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%%%

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
\shorttitle{Leveraging social media news}
\shortauthors{CV Radhakrishnan et~al.}

\title [mode = title]{Fingerprint Reconstruction Using a Convolutional Generative Adversarial Model}                     
\tnotemark[1,2]

\tnotetext[1]{This document is the results of the research project funded by the National Science Foundation.}
\tnotetext[2]{The second title footnote which is a longer text matter to fill through the whole text width and overflow into another line in the footnotes area of the first page.}

% AUTHOR 1
\author[1,3]{Cristian Yesid Andrade Hern√°ndez}
[type=editor,
auid=000,bioid=1,
prefix=Sir,
role=Student,
orcid=0000-0001-7511-2910]
\cormark[1]
\fnmark[1]
\ead{cy.andrade@uniandes.edu.co}
\ead[url]{www.cvr.cc, cvr@sayahna.org}
\credit{Conceptualization of this study, Methodology, Software}
\address[1]{Elsevier B.V., Radarweg 29, 1043 NX Amsterdam, The Netherlands}

% AUTHOR 2
\author[2,4]{Han Theh Thanh}
[style=chinese]

% AUTHOR 3
\author[2,3]{CV Rajagopal}
[role=Co-ordinator,
suffix=Jr,]
\fnmark[2]
\ead{cvr3@sayahna.org}
\ead[URL]{www.sayahna.org}
\credit{Data curation, Writing - Original draft preparation}
\address[2]{Sayahna Foundation, Jagathy, Trivandrum 695014, India}

% AUTHOR 4
\author
[1,3]{Rishi T.}
\cormark[2]
\fnmark[1,3]
\ead{rishi@stmdocs.in}
\ead[URL]{www.stmdocs.in}
\address[3]{STM Document Engineering Pvt Ltd., Mepukada, Malayinkil, Trivandrum 695571, India}

\cortext[cor1]{Corresponding author}
\cortext[cor2]{Principal corresponding author}
\fntext[fn1]{This is the first author footnote. but is common to third author as well.}
\fntext[fn2]{Another author footnote, this is a very long footnote and it should be a really long footnote. But this footnote is not yet sufficiently long enough to make two lines of footnote text.}

\nonumnote{This note has no numbers. In this work we demonstrate $a_b$ the formation Y\_1 of a new type of polariton on the interface between a cuprous oxide slab and a polystyrene micro-sphere placed on the slab.}

\begin{abstract}
Biometric systems record fingerprints into digital platforms for allowing governments and organizations to have a structured and reliable way to identify people. In some cases, uncontrolled factors in both enrollment and verification processes make the biometric systems to obtain poor quality fingerprints records. thus, performance of automatic identification decreases and the work of dactyloscopists becomes harder. This paper describes the implementation of a convolutional generative adversarial model that performs fingerprint image reconstruction in order to obtain clear ridge patterns using TensorFlow. fingerprint enhancement boosts the correct extraction of fingerprint features called minutiae which are the center of matching and identification algorithms. A biometric open source framework called NBIS is used to measure the efectiveness of the model in terms of matching accuracy and image quality.
\end{abstract}

\begin{graphicalabstract}
\includegraphics{figs/grabs.pdf}
\end{graphicalabstract}

\begin{highlights}
\item Research highlights item 1
\item Research highlights item 2
\item Research highlights item 3
\end{highlights}

\begin{keywords}
fingerprint \sep enhancement \sep generative \sep adversarial \sep convolutional \sep neural \sep network
\end{keywords}


\maketitle

\section{Introduction}

The minutia based algorithms are the most widely used for fingerprint recognition. Minutiae are local ridge and valley characteristics of fingerprints such as endings and bifurcations \cite{HFPR}. Each minutia is determined by four values: pixel position in vertical axis, pixel position in horizontal axis, orientation and quality \cite{NBISUG}. The algorithm computes a score based on the minutia of two fingerprints and if the score is above a certain threshold the fingerprints are determined to belong to the same person.

There are cases where quality of recorded fingerprints is poor due to factors like cut, wet, dry or burnt fingers, skin diseases, dirty scanning surfaces, incorrect pressure while scanning, among others. These factors remove real minutia or create false ones affecting the matching process. Fingerprint reconstruction and enhancement allow the algorithms to distinguish correctly between ridges and valleys, so real minutia is detected and identification accuracy improves.

The solution proposed implements the Pix2Pix model which is a generative adversarial convolutional neural network that solves the image to image translation problem. It consists of a set of interconnected layers that receives an image as input, process it with convolutions and return the expected image as output. The parameters of the layer's filters are modified through an optimization process that minimize a cost function using a similarity measure and the adversarial training paradigm. 

Two main frameworks were used to develop the project. TensorFlow machine learning platform was used to build and train the model. The second one is NIST Biometric Image Software (NBIS) an open source biometric framework with core capabilities on fingerprint image processing developed by the National Institute of Standards and Technology (NIST) for the Federal Bureau of Investigation (FBI) and Department of Homeland Security (DHS) \cite{NBISWP}. Three modules were used in the project: \textit{mindtct} to extract minutiae in format \textit{xyt} from a fingerprint, \textit{bozorth3} to compute a similarity score between either a pair or a list of pairs of minutiae using the bozorth algorithm and \textit{nfiq} to return a quality measure of digital fingerprints.

The paper follows the next organization: the source and preprocessing of training data is described in section \hyperref[sec:DP]{2}. Section \hyperref[sec:MA]{3} presents model architecture details. Training configuration is explained in section \hyperref[sec:MT]{4}. Section \hyperref[sec:R]{5} presents and analyses results. Finally, conclusion and future work are proposed in section \hyperref[sec:FW]{6} and \hyperref[sec:FW]{7} respectively.
     
\section{Data preprocessing}
\label{sec:DP}

Training data came from two sources. The first one is a private dataset of 19.859 fingerprints recorded within an enrollment process of citizen procedures. The second corresponds to 39.611 real fingerprints from LivDet dataset, a dataset built for a competition that rewards the best spoof detection model. Fig.~\ref{fig1} shows some examples of the datasets. 

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{images/-.png}}
\caption{Example of images from source datasets}
\label{fig1}
\end{figure}

As data comes from different sources, the first step of preprocessing is to resize images into a resolution of 256x256. The next step is to build tuples, each one conformed by an image to enhance, and a target image. The image to enhance is obtained through an artificial deterioration process and the target image is the original image of the dataset. There are two types of deterioration. The first one consists in drawing noisy holes to simulate a skin disease, a burnt finger or other types of fingerprint modifications. It is achieved generating random white ellipses and summing a tiny gaussian noise as can be seen in Fig.~\ref{fig2}.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{images/-.png}}
\caption{Artificial fingerprint deterioration with holes}
\label{fig2}
\end{figure}

The second type of deterioration is oriented to simulate wet or dry fingers or noise or dirty scanning surfaces. This is achieved by summing a specific value to every pixel in the image until the mean of the image reaches 250. This removes some ridges and blurs the image. Finally, a gaussian noise is summed as it is showed in Fig.~\ref{fig3}.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.32]{images/-.png}}
\caption{Artificial fingerprint deterioration simulating finger or scanning modified conditions}
\label{fig3}
\end{figure}

Once the data is loaded and preprocessed, tuples are feeded into the model for making it to learn the optimum transformation that results in restored fingerprint images.

\section{Model Architecture}
\label{sec:MA}

Proposed architecture follows the guidelines of an image to image translation model called Pix2Pix \cite{ITITAN}. It is composed by a generator that receives the image to enhance as input and returns the reconstructed image as output and a discriminator that determines whether or not an image was successfully reconstructed. Both convolutional structures will be detailed next.

Generator consists in a encoder and a decoder that configure an U-Net structure \cite{UNBIS}. The encoder is made up of eight sequentially connected convolutional layers that reduce the input height and width by a factor of two using same padding and strides of two. All layers use leakyrelu activation and batch normalization except for the first one. Similarly, decoder is made up of eight transpose convolutional layers, sometimes mistakenly called deconvolutional layers. In contrast to encoder, decoder layers increment by a factor of two height and width of input volumes which results in a final image of 256x256 pixels. All layers use batch normalization and relu activation and the first three layers use drop regularization. Fig.~\ref{fig4} shows a sketch of the generator and Table~\ref{tab1} of appendix specifies architecture details including number of filters on each layer.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.37]{images/-.png}}
\caption{Generator architecture}
\label{fig4}
\end{figure}

Discriminator is similar to a encoder since it applies convolutions to extract features from the input. However, unlike standard GANs, discriminator maps the input fingerprint to a 29x29 logits square rather than to a scalar for deciding whether or not the image was correctly enhanced. Sigmoid function is applied to 29x29 output and then it is weighted to obtain the final value for the discriminator decision. This is called a PathGAN because each value of the 29x29 output, approximately corresponds to a 70x70 patch of the input image. Fig.~\ref{fig5} shows a sketch of the generator and Table~\ref{tab2} of appendix specifies architecture details including number of filters on each layer.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.4]{images/-.png}}
\caption{Discriminator architecture}
\label{fig5}
\end{figure}

\section{Model training}
\label{sec:MT}

Let's recall that the main objective is make the generator to reconstruct and enhance deteriorate fingerprints the best way as possible. To achieve that, the model is trained through a optimization process that uses back propagation to calculate gradients and adam optimizer to update parameters. Since it is an adversarial architecture, there are two cost functions to minimize that will be detailed next.

The first cost function refers to the generator and it is made up of the sum of two terms. To obtain the first term, a deteriorated fingerprint of the dataset is passed through the generator. The reconstructed image is then compared to its ground truth using L1 norm. Second term is obtained concatenating reconstructed and deteriorated image, then passing the result through the discriminator and finally calculating cross entropy with label equal to one meaning the intent of generator to fool the discriminator. Next equation summarizes what was previously described.
\begin{equation}
Gen_{loss} = \alpha||y-gen(x)||_{[1]} - log(disc(x,gen(x)))
\end{equation}

Where \textit{x} corresponds to a deteriorated image, \textit{gen(x)} the reconstruction, \textit{y} the ground truth image, \textit{disc(x,gen(x))} the probability calculated passing the enhanced fingerprint through the discriminator and $\alpha$ defines the importance of reconstruction term. The deteriorated image is concatenated to the enhanced image in order to give a context that helps training.

The second cost function corresponds to counterpart of generator, the discriminator. It is also made up of the sum of two terms: the first is obtained concatenating reconstructed and deteriorated image, then passing the result through the discriminator and finally computing cross entropy with label equal to zero meaning the intent of discriminator of not being fooled by the generator. The second term is calculated similarly, however, it is concatenated the ground truth fingerprint instead of the reconstructed one and label is equal to one indicating that ground truth image is the real reconstructed.
\begin{equation}
Disc_{loss} = -log(1-disc(x,gen(x)))-log(disc(x,y))
\end{equation}

Training is performed by TensorFlow framework using a Nvidia Tesla K40 graphic card and it is configured as follows. Tuples formed in the preprocessing step are grouped into batches of 48 elements. Adam optimizer is configured with $\beta_{1}=0.5$, $\beta_{2}=0.999$ and a learning rate of $0.00018$.Reconstruction $\alpha$ is set to 50.

\section{Results}
\label{sec:R}

Fig.~\ref{fig6} shows enhancement of three fingerprints using the proposed method and the first type of deterioration. The model was able to reconstruct the corrupted regions drawing the ridges that had been disappeared.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{images/-.png}}
\caption{ROC curve comparison before and after reconstruction}
\label{fig6}
\end{figure}

The reconstruction of fingerprints altered with the second type of deterioration is shown in Fig.~\ref{fig7}. The most of the ridges were recovered and the noise was diminished improving clarity and quality of the fingerprint.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{images/-.png}}
\caption{ROC curve comparison before and after reconstruction}
\label{fig7}
\end{figure}

In contrast to previous results, there were some fingerprint examples that could not be restored effectively as shown in Fig.~\ref{fig8}. In those cases the deterioration caused a high level of corruption in the fingerprints. Therefore, it was almost impossible to achieve a successful enhancement.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{images/-.png}}
\caption{ROC curve comparison before and after reconstruction}
\label{fig8}
\end{figure}

The first test was carried out using the Receiver Operating Characteristic (ROC), a well-accepted measure to express performance of 1:1 matchers \cite{RROCCMC}. The matching scores between images to reconstruct and the ground truth images and between reconstructed images and ground truth images were computed. These scores are compared to a threshold to determine whether or not the pair belongs to the same person. finally, the CMC curve is graphed plotting the False Accept Rate(FAR) and False Reject Rate (FRR) obtained for different thresholds. Fig.~\ref{fig9} shows achieved results.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{images/-.png}}
\caption{ROC curve comparison before and after reconstruction}
\label{fig9}
\end{figure}

Analysis ROC curve.

The Cumulative Match Curve (CMC) is used in the second test. Since it is a 1:m matcher evaluator the matching score must be computed on every possible tuple formed by a deteriorated image and a ground truth image and a enhanced image and a ground truth image. The curves are obtained following steps described in \cite{RROCCMC}. Fig.~\ref{fig10} shows obtained graph.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{images/-.png}}
\caption{CMC curve comparison before and after reconstruction}
\label{fig10}
\end{figure}

Analysis CMC curve.

The final performance metric consists on measuring average fingerprint image quality of validation dataset before and after reconstruction. Results are shown in Fig.~\ref{fig11}

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{images/-.png}}
\caption{Quality mean curve comparison before and after reconstruction}
\label{fig11}
\end{figure}

Analysis MEAN curve.

\section{Conclusion}
\label{sec:CON}

The generative model demonstrated a good performance on its reconstruction task. Both ROC and CMC curves showed an improvement after the enhancement process and the fingerprints quality as well. It is worth mentioning that filters use pixels context around corrupted regions to reconstruct them. Therefore, if the deteriorated region is too big or corrupted, model is not able to make a successful reconstruction. 

\section{Future work}
\label{sec:FW}

More complex deterioration algorithms can be studied in order to simulate real world scenarios which can result in a more accurate model. Additionally, in order to improve algorithm generalization and performance it can be developed a dataset containing real fingerprint corruptions and its corresponding ground truths.

\appendix
\section{My Appendix}
Appendix sections are coded under \verb+\appendix+.

\verb+\printcredits+ command is used after appendix sections to list 
author credit taxonomy contribution roles tagged using \verb+\credit+ 
in frontmatter.

\printcredits

%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
\bibliographystyle{cas-model2-names}

% Loading bibliography database
\bibliography{cas-refs}

%\vskip3pt

\bio{}
Author biography without author photo.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
\endbio

\bio{figs/pic1}
Author biography with author photo.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
\endbio

\bio{figs/pic1}
Author biography with author photo.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
Author biography. Author biography. Author biography.
\endbio

\end{document}